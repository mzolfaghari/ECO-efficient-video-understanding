#ifdef USE_CUDNN
#include <algorithm>
#include <vector>

#include "caffe/filler.hpp"
#include "caffe/layer.hpp"
#include "caffe/util/im2col.hpp"
#include "caffe/util/math_functions.hpp"
#include "caffe/vision_layers.hpp"

#include <boost/unordered_map.hpp>
#include <cudnn.h>

using boost::unordered_map;

namespace caffe {

// Set to three for the benefit of the backward pass, which
// can use separate streams for calculating the gradient w.r.t.
// bias, filter weights, and bottom data for each group independently
#define CUDNN_FWD_STREAMS_PER_GROUP 1
#define CUDNN_BWD_STREAMS_PER_GROUP 2

template <typename Dtype>
shared_ptr<SyncedMemory> CuDNNConvolutionLayer<Dtype>::workspaceData_fwd;
template <typename Dtype>
shared_ptr<SyncedMemory> CuDNNConvolutionLayer<Dtype>::workspaceData_bwd_filter;
template <typename Dtype>
shared_ptr<SyncedMemory> CuDNNConvolutionLayer<Dtype>::workspaceData_bwd_data;

template <typename Dtype>
size_t CuDNNConvolutionLayer<Dtype>::conv_layer_count = 0;


/**
 * TODO(dox) explain cuDNN interface
 */
template <typename Dtype>
void CuDNNConvolutionLayer<Dtype>::LayerSetUp(
    const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top) {
  ConvolutionLayer<Dtype>::LayerSetUp(bottom, top);

  if (conv_layer_count == 0){
    workspaceData_fwd = shared_ptr<SyncedMemory>(new SyncedMemory());
    workspaceData_bwd_filter = shared_ptr<SyncedMemory>(new SyncedMemory());
    workspaceData_bwd_data = shared_ptr<SyncedMemory>(new SyncedMemory());
  }
  conv_layer_count++;

  // Initialize CUDA streams and cuDNN.
  int total_streams_per_group = CUDNN_FWD_STREAMS_PER_GROUP + CUDNN_BWD_STREAMS_PER_GROUP;
  stream_         = new cudaStream_t[this->group_ * total_streams_per_group];
  handle_         = new cudnnHandle_t[this->group_ * total_streams_per_group];


  // initialize size arrays
  workspace_fwd_offsets_ = new size_t[bottom.size()];
  workspace_bwd_filter_offsets_ = new size_t[bottom.size()];
  workspace_bwd_data_offsets_ = new size_t[bottom.size()];


  for (size_t i = 0; i < bottom.size(); ++i) {
    // initialize all to default algorithms
    fwd_algo_.push_back((cudnnConvolutionFwdAlgo_t)0);
    bwd_filter_algo_.push_back((cudnnConvolutionBwdFilterAlgo_t)0);
    bwd_data_algo_.push_back((cudnnConvolutionBwdDataAlgo_t)0);
    // default algorithms don't require workspace
    workspace_fwd_offsets_[i] = 0;
    workspace_bwd_filter_offsets_[i] = 0;
    workspace_bwd_data_offsets_[i] = 0;
  }

  for (int g = 0; g < this->group_ * total_streams_per_group; g++) {
    CUDA_CHECK(cudaStreamCreate(&stream_[g]));
    CUDNN_CHECK(cudnnCreate(&handle_[g]));
    CUDNN_CHECK(cudnnSetStream(handle_[g], stream_[g]));
  }

  // Set the indexing parameters.

  bias_offset_ = (this->num_output_ / this->group_);

  std::vector<int> kernel_shape;
  kernel_shape.push_back(this->num_output_ / this->group_);
  kernel_shape.push_back(this->channels_ / this->group_);
  for (unsigned int i = 0; i < this->num_spatial_axes_; ++i)
    kernel_shape.push_back(this->kernel_shape_.cpu_data()[i]);

  cudnn::createNdFilterDesc<Dtype>(&filter_desc_, kernel_shape);

  // Create tensor descriptor(s) for data and corresponding convolution(s).
  for (int i = 0; i < bottom.size(); i++) {
    cudnnTensorDescriptor_t bottom_desc;
    cudnn::createTensorDesc<Dtype>(&bottom_desc);
    bottom_descs_.push_back(bottom_desc);
    cudnnTensorDescriptor_t top_desc;
    cudnn::createTensorDesc<Dtype>(&top_desc);
    top_descs_.push_back(top_desc);
    cudnnConvolutionDescriptor_t conv_desc;
    cudnn::createConvolutionDesc<Dtype>(&conv_desc);
    conv_descs_.push_back(conv_desc);
  }

  // Tensor descriptor for bias.
  if (this->bias_term_) {
    cudnn::createTensorDesc<Dtype>(&bias_desc_);
  }

  handles_setup_ = true;
  need_benchmark_ = true;

}

template <typename Dtype>
void CuDNNConvolutionLayer<Dtype>::Reshape(
    const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top) {
  ConvolutionLayer<Dtype>::Reshape(bottom, top);

  bottom_offset_ = this->bottom_dim_ / this->group_;
  top_offset_ = this->top_dim_ / this->group_;

  std::vector<int> bottom_tensor_shape(bottom[0]->shape());
  bottom_tensor_shape[1] /= this->group_;
  std::vector<int> bottom_tensor_stride(bottom[0]->shape().size(), 1);
  for (int i = bottom[0]->shape().size() - 2; i >= 0; --i) {
    bottom_tensor_stride[i] =
        bottom[0]->shape(i + 1) * bottom_tensor_stride[i + 1];
  }

  std::vector<int> top_tensor_shape(top[0]->shape());
  top_tensor_shape[1] /= this->group_;
  std::vector<int> top_tensor_stride(top[0]->shape().size(), 1);
  for (int i = top[0]->shape().size() - 2; i >= 0; --i) {
    top_tensor_stride[i] = top[0]->shape(i + 1) * top_tensor_stride[i + 1];
  }

  std::vector<int> pad, stride;
  for (unsigned int i = 0; i < this->num_spatial_axes_; ++i) {
    pad.push_back(this->pad_.cpu_data()[i]);
    stride.push_back(this->stride_.cpu_data()[i]);
  }
  // Specify workspace limit for kernels directly until we have a
  // planning strategy and a rewrite of Caffe's GPU memory mangagement.
  //
  // However this can be tuned by the "richness" parameter in the solver protobuf
  // By setting richness, you can increase the memory available to cuDNN and thus
  // let it choose fast but space consuming algorithms.
  for (int i = 0; i < bottom.size(); i++) {


    cudnn::setTensorNdDesc<Dtype>(&bottom_descs_[i],
        bottom_tensor_shape, bottom_tensor_stride);
    cudnn::setTensorNdDesc<Dtype>(&top_descs_[i],
        top_tensor_shape, top_tensor_stride);
    cudnn::setNdConvolutionDesc<Dtype>(&conv_descs_[i], bottom_descs_[i],
        filter_desc_, pad, stride);

  if (need_benchmark_){
      // choose forward and backward algorithms + workspace(s)
      const int kRequestedForwardAlgoCount = 6;
      vector<cudnnConvolutionFwdAlgoPerf_t> fwd_perf;
      fwd_perf.resize(kRequestedForwardAlgoCount);
      int returnedAlgoCount;
      size_t mem_limit = 200*1024*1024;
      CUDNN_CHECK(cudnnGetConvolutionForwardAlgorithm (handle_[0], bottom_descs_[i], filter_desc_, conv_descs_[i], top_descs_[i],CUDNN_CONVOLUTION_FWD_SPECIFY_WORKSPACE_LIMIT,mem_limit, &fwd_algo_[i]));
/*
      CUDNN_CHECK(cudnnGetConvolutionForwardWorkspaceSize(handle_[0],
                                                       bottom_descs_[i],
                                                       filter_desc_,
                                                       conv_descs_[i],
                                                       top_descs_[i],
                                                       kRequestedForwardAlgoCount,
                                                       &returnedAlgoCount,
                                                       &fwd_perf[0]));

      // choose the fastest within limit
      // if all algorithms exceed memory limit, we will use the 0 algorithm with no workspace
      for (int a = 0; a < kRequestedForwardAlgoCount; ++a){
        if (fwd_perf[a].memory * this->group_ < (Caffe::cudnn_mem_richness() * 1024 * 1024)
            || Caffe::cudnn_mem_richness() == 0){
          fwd_algo_[i] = fwd_perf[a].algo;
          break;
        }
      }
*/

      // choose backward algorithm for filter
      const int kRequestedBackwardFilterAlgoCount = 4;
      vector<cudnnConvolutionBwdFilterAlgoPerf_t> bwd_filter_perf;
      bwd_filter_perf.resize(kRequestedBackwardFilterAlgoCount);
      CUDNN_CHECK(cudnnGetConvolutionBackwardFilterAlgorithm(handle_[0], bottom_descs_[i], top_descs_[i], conv_descs_[i], filter_desc_,
                                                                   CUDNN_CONVOLUTION_BWD_FILTER_SPECIFY_WORKSPACE_LIMIT,
                                                                   mem_limit,
                                                                   &bwd_filter_algo_[i]));

 
/*
      CUDNN_CHECK(cudnnFindConvolutionBackwardFilterAlgorithm(handle_[0],
                                                              bottom_descs_[i],
                                                              top_descs_[i],
                                                              conv_descs_[i],
                                                              filter_desc_,
                                                              kRequestedBackwardFilterAlgoCount,
                                                              &returnedAlgoCount,
                                                              &bwd_filter_perf[0]));

      // choose the fastest within limit
      // if all algorithms exceed memory limit, we will use the 0 algorithm with no workspace
      for (int a = 0; a < kRequestedBackwardFilterAlgoCount; ++a){
        if (bwd_filter_perf[a].memory * this->group_ < (Caffe::cudnn_mem_richness() * 1024 * 1024)
            || Caffe::cudnn_mem_richness() == 0){
          bwd_filter_algo_[i] = bwd_filter_perf[a].algo;
          break;
        }
      }

*/
      // choose backward algo for data
      const int kRequestedBackwardDataAlgoCount = 4;
      vector<cudnnConvolutionBwdDataAlgoPerf_t> bwd_data_perf;
      bwd_data_perf.resize(kRequestedBackwardDataAlgoCount);

           //backward data
            CUDNN_CHECK(cudnnGetConvolutionBackwardDataAlgorithm(handle_[0],
                                                                 filter_desc_, top_descs_[i], conv_descs_[i], bottom_descs_[i],
                                                                 CUDNN_CONVOLUTION_BWD_DATA_SPECIFY_WORKSPACE_LIMIT,
                                                                 mem_limit,
                                                                 &bwd_data_algo_[i]));
/*
      CUDNN_CHECK(cudnnFindConvolutionBackwardDataAlgorithm(handle_[0],
                                                            filter_desc_,
                                                            top_descs_[i],
                                                            conv_descs_[i],
                                                            bottom_descs_[i],
                                                            kRequestedBackwardDataAlgoCount,
                                                            &returnedAlgoCount,
                                                            &bwd_data_perf[0]));

      // choose the fastest within limit
      // if all algorithms exceed memory limit, we will use the 0 algorithm with no workspace
      for (int a = 0; a < kRequestedBackwardDataAlgoCount; ++a){
        if (bwd_data_perf[a].memory * this->group_ <(Caffe::cudnn_mem_richness() * 1024 * 1024)
            || Caffe::cudnn_mem_richness() == 0){
          bwd_data_algo_[i] = bwd_data_perf[a].algo;
          break;
        }
      }
*/

      need_benchmark_ = false;
    }
  }


  // Tensor descriptor for bias.
  if (this->bias_term_) {

    vector<int> bias_shape(bottom[0]->shape().size(), 1);
    bias_shape[1] = this->num_output_ / this->group_;
    cudnn::setTensorNdDesc<Dtype>(&bias_desc_, bias_shape);
  }

  AdjustWorkSpaces();
}

template<typename Dtype>
void CuDNNConvolutionLayer<Dtype>::AdjustWorkSpaces() {

  size_t workspace_size_fwd = 0;
  size_t workspace_size_bwd_data = 0;
  size_t workspace_size_bwd_filter = 0;

  for (int i = 0; i < fwd_algo_.size(); ++i){
    size_t workspace_size;
    cudnnGetConvolutionForwardWorkspaceSize(handle_[0],
                                            bottom_descs_[i], filter_desc_,
                                            conv_descs_[i],
                                            top_descs_[i],
                                            fwd_algo_[i], &workspace_size);
    workspace_fwd_offsets_[i] = workspace_size;
    workspace_size_fwd = std::max(workspace_size * this->group_, workspace_size_fwd);

    cudnnGetConvolutionBackwardFilterWorkspaceSize(handle_[1],
                                                   bottom_descs_[i], top_descs_[i],
                                                   conv_descs_[i],
                                                   filter_desc_,
                                                   bwd_filter_algo_[i], &workspace_size);
    workspace_bwd_filter_offsets_[i] = workspace_size;
    workspace_size_bwd_filter = std::max(workspace_size * this->group_, workspace_size_bwd_filter);

    cudnnGetConvolutionBackwardDataWorkspaceSize(handle_[2],
                                                 filter_desc_,
                                                 top_descs_[i],
                                                 conv_descs_[i],
                                                 bottom_descs_[i],
                                                 bwd_data_algo_[i], &workspace_size);
    workspace_bwd_data_offsets_[i] = workspace_size;
    workspace_size_bwd_data = std::max(workspace_size * this->group_, workspace_size_bwd_data);
  }

  workspaceData_fwd->Resize(workspace_size_fwd);
  workspaceData_bwd_filter->Resize(workspace_size_bwd_filter);
  workspaceData_bwd_data->Resize(workspace_size_bwd_data);
}

template <typename Dtype>
CuDNNConvolutionLayer<Dtype>::~CuDNNConvolutionLayer() {
  // Check that handles have been setup before destroying.
  if (!handles_setup_) { return; }

  for (int i = 0; i < bottom_descs_.size(); i++) {
    cudnnDestroyTensorDescriptor(bottom_descs_[i]);
    cudnnDestroyTensorDescriptor(top_descs_[i]);
    cudnnDestroyConvolutionDescriptor(conv_descs_[i]);
  }
  if (this->bias_term_) {
    cudnnDestroyTensorDescriptor(bias_desc_);
  }
  cudnnDestroyFilterDescriptor(filter_desc_);

  int total_stream_per_group = CUDNN_FWD_STREAMS_PER_GROUP + CUDNN_BWD_STREAMS_PER_GROUP;
  for (int g = 0; g < this->group_ * total_stream_per_group; g++) {
    cudaStreamDestroy(stream_[g]);
    cudnnDestroy(handle_[g]);
  }

  --conv_layer_count;
  if (conv_layer_count == 0){
    workspaceData_fwd.reset();
    workspaceData_bwd_filter.reset();
    workspaceData_bwd_data.reset();
  }

  delete [] stream_;
  delete [] handle_;
  delete [] workspace_fwd_offsets_;
  delete [] workspace_bwd_data_offsets_;
  delete [] workspace_bwd_filter_offsets_;

}

INSTANTIATE_CLASS(CuDNNConvolutionLayer);

}   // namespace caffe
#endif
